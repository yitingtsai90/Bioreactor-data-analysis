{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OTU data\n",
    "df = pd.read_csv(open('df_all.csv','rb'),skip_blank_lines=True)\n",
    "\n",
    "# Add 1 to all counts and log10-transform\n",
    "df_plus = df.copy()\n",
    "df_plus.loc[:, 'Otu8':'Otu819'] = df.loc[:, 'Otu8':'Otu819'] + 1\n",
    "df_log10 = df_plus.copy()\n",
    "df_log10.loc[:, 'Otu8':'Otu819'] = np.log10(df_plus.loc[:, 'Otu8':'Otu819'])\n",
    "\n",
    "# Grab original OTU labels\n",
    "OTU_columns = df_plus.loc[:, 'Otu8':'Otu819'].columns\n",
    "\n",
    "# Import clustering results\n",
    "cluster_tally = pd.read_csv('cluster_tally.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ff602a55773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import representative OTUs from hierarchical clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclosest_OTUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"closest_OTUs.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Grab log10 counts of representative OTUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrep_OTUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_log10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_OTUs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Import representative OTUs from hierarchical clustering\n",
    "closest_OTUs = np.load(\"closest_OTUs.npy\")\n",
    "\n",
    "# Grab log10 counts of representative OTUs\n",
    "rep_OTUs = df_log10[closest_OTUs]\n",
    "\n",
    "# Append the SampleID column to the left\n",
    "rep_OTUs.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)\n",
    "\n",
    "# Standardize these representative OTU counts\n",
    "rep_OTUs_scaled = rep_OTUs.copy()\n",
    "rep_OTUs_scaled = rep_OTUs_scaled.drop(labels=['SampleID'],axis=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "rep_OTUs_scaled = pd.DataFrame(scaler.fit_transform(rep_OTUs_scaled))\n",
    "rep_OTUs_scaled.columns = rep_OTUs.columns[1:]\n",
    "\n",
    "# # Sanity check after standardization:\n",
    "# print(\"Averages are: \\n{} \\n\".format(np.mean(rep_OTUs_scaled,axis=0))) # Sanity check\n",
    "# print(\"Standard deviations are: \\n{} \\n\".format(np.std(rep_OTUs_scaled,axis=0))) # Sanity check\n",
    "\n",
    "# Finally add back the SampleID column\n",
    "rep_OTUs_scaled.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import representative OTUs from Gaussian clustering\n",
    "closest_OTUs_Gaussian = np.load(\"closest_OTUs_Gaussian.npy\")\n",
    "\n",
    "# Grab log10 counts of representative OTUs\n",
    "rep_OTUs_Gaussian = df_log10[closest_OTUs_Gaussian]\n",
    "\n",
    "# Append the SampleID column to the left\n",
    "rep_OTUs_Gaussian.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)\n",
    "\n",
    "# Standardize these representative OTU counts\n",
    "rep_OTUs_Gaussian_scaled = rep_OTUs_Gaussian.copy()\n",
    "rep_OTUs_Gaussian_scaled = rep_OTUs_Gaussian_scaled.drop(labels=['SampleID'],axis=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "rep_OTUs_Gaussian_scaled = pd.DataFrame(scaler.fit_transform(rep_OTUs_Gaussian_scaled))\n",
    "rep_OTUs_Gaussian_scaled.columns = rep_OTUs_Gaussian.columns[1:]\n",
    "\n",
    "# Finally add back the SampleID column\n",
    "rep_OTUs_Gaussian_scaled.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import representative OTUs from Dirichlet clustering\n",
    "closest_OTUs_Dirichlet = pd.read_csv('Dirichlet_OTUs.txt',sep='\\s',engine='python').columns\n",
    "\n",
    "# Grab log10 counts of representative OTUs\n",
    "rep_OTUs_Dirichlet = df_log10[closest_OTUs_Dirichlet]\n",
    "\n",
    "# Append the SampleID column to the left\n",
    "rep_OTUs_Dirichlet.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)\n",
    "\n",
    "# Standardize these representative OTU counts\n",
    "rep_OTUs_Dirichlet_scaled = rep_OTUs_Dirichlet.copy()\n",
    "rep_OTUs_Dirichlet_scaled = rep_OTUs_Dirichlet_scaled.drop(labels=['SampleID'],axis=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "rep_OTUs_Dirichlet_scaled = pd.DataFrame(scaler.fit_transform(rep_OTUs_Dirichlet_scaled))\n",
    "rep_OTUs_Dirichlet_scaled.columns = rep_OTUs_Dirichlet.columns[1:]\n",
    "\n",
    "# Finally add back the SampleID column\n",
    "rep_OTUs_Dirichlet_scaled.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the OTUs according to their clusters, then sum up the log10 counts:\n",
    "df_log10_sums = pd.DataFrame(index=df_log10.index,columns=cluster_tally.index)\n",
    "\n",
    "for i in df_log10_sums.index:\n",
    "    rel_row = df_log10.loc[[i]]\n",
    "    for j in df_log10_sums.columns:\n",
    "        # Grab members corresponding to group j\n",
    "        members = cluster_tally.loc[[j]].dropna(axis='columns',how='any').values\n",
    "        # Sum up their log10 abundance counts\n",
    "        counts_sum = 0\n",
    "        for m in range(0,members.shape[1]):\n",
    "            OTU_tag = members[0,m]\n",
    "            counts_sum = counts_sum + rel_row.loc[:,OTU_tag].values\n",
    "        df_log10_sums.loc[[i],j] = counts_sum\n",
    "\n",
    "# Finally, append the SampleID column to the left\n",
    "df_log10_sums.insert(loc=0, column='SampleID', value=df_log10.loc[:,'SampleID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the clustered OTU data with waterchem data\n",
    "# Grab waterchem data\n",
    "df_waterchem = pd.read_csv(open('df_waterchem.csv','rb'),skip_blank_lines=True)\n",
    "\n",
    "# Append OTU data to the right\n",
    "df_everything = pd.merge(df_waterchem,rep_OTUs_scaled, how='left', left_on=['SampleID'], right_on=['SampleID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sanity check to ensure every column is standardized\n",
    "print(\"Averages are: \\n{} \\n\".format(np.mean(df_everything,axis=0))) # Sanity check\n",
    "print(\"Standard deviations are: \\n{} \\n\".format(np.std(df_everything,axis=0))) # Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MDA for benchmark case (water chem variables only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_everything.drop(labels=['SampleID','NRR','NRF','SeRR','SeRF'],axis=1)\n",
    "y = pd.DataFrame(df_everything['SeRR'])\n",
    "n_feats = X.columns.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the waterchem variables?\n",
    "waterchem_var = X_train_water.columns\n",
    "\n",
    "# How many waterchem variables are there?\n",
    "num_waterchem_var = waterchem_var.shape[0]\n",
    "\n",
    "print(num_waterchem_var)\n",
    "print(waterchem_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each model we train corresponds to a randomly selected training/testing split, as well as a random permutation of one variable. Therefore we need to repeat this procedure $n_{trials}$ times and average over them to get a reasonable estimate of the MDA values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Base-case model with no variables permutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, specify the number of MDA experiments to perform:\n",
    "n_MDA_runs = 10\n",
    "\n",
    "shuffled_accuracies = np.zeros([num_waterchem_var,n_MDA_runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify desired test fraction:\n",
    "test_frac = 0.4 # No hyperparameter selection, so no validation set\n",
    "\n",
    "# Obtain categorical values of y: (y is 0 if positive, 1 if negative)\n",
    "y['class'] = np.where(y['SeRR']>=0, 0, 1)\n",
    "y_class = y['class'].values\n",
    "C = np.unique(y_class).shape[0] # Number of classes\n",
    "\n",
    "y_onehot = np.eye(C)[y_class.astype(int)] # Convert categorical to one-hot-encoding\n",
    "y_onehot = y_onehot.reshape(y_class.shape[0],C) # Reshape into dimensions (n_t rows by C columns)\n",
    "\n",
    "model_num = 0\n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    ### STEP 0: Randomly split data into training and testing.\n",
    "\n",
    "    # Split into training and testing portions\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y_onehot,test_size=test_frac,shuffle=True)\n",
    "\n",
    "    ## Now grab the waterchem data input subset:\n",
    "    X_train_water = X_train.loc[:,'EBCT':'FBR3']\n",
    "    X_test_water = X_test.loc[:,'EBCT':'FBR3']\n",
    "\n",
    "    acc_array = []\n",
    "\n",
    "    ## For TRAINING SET:\n",
    "    # Shuffle the values of this variable arbitrarily:\n",
    "    values_to_shuffle = X_train_water.loc[:,permutate_var].values\n",
    "    values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "    # Put these shuffled values back in the original dataset\n",
    "    X_train_water_shuffled = X_train_water.copy()\n",
    "    X_train_water_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "    ## For TESTING SET:\n",
    "    # Shuffle the values of this variable arbitrarily:\n",
    "    values_to_shuffle = X_test_water.loc[:,permutate_var].values\n",
    "    values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "    # Put these shuffled values back in the original dataset\n",
    "    X_test_water_shuffled = X_test_water.copy()\n",
    "    X_test_water_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "### STEP 2: RE-TRAIN ANN USING SHUFFLED DATASET, THEN EVALUATE ACCURACY:\n",
    "\n",
    "    # Train ANN:\n",
    "    X_ANN = tf.placeholder(tf.float32,shape=[None,X_train_water_shuffled.shape[1]])\n",
    "    y_true_ANN = tf.placeholder(tf.float32,shape=[None,C])\n",
    "\n",
    "    # Specify hyperparameters\n",
    "    npl = 20 # Number of neurons per layer\n",
    "    n_hidden = 10 # Number of hidden layers\n",
    "    lrate = 0.01 # Learning rate for gradient descent\n",
    "    epochs = 10 # Total number of iterations\n",
    "    spe = 20 # Steps per epoch: One step represents one update to the cost function gradient\n",
    "    actf = tf.nn.relu # Activation function\n",
    "    alpha = 0.1 # Magnitude of regularizer\n",
    "    reg = tf.contrib.layers.l2_regularizer(scale=alpha) # Regularizer function\n",
    "\n",
    "    dimof_output = C # Dimension of y\n",
    "\n",
    "    ## Specify neural net architecture\n",
    "\n",
    "    # Use a dictionary setup to generalize number of layers\n",
    "    hlayer = dict()\n",
    "    hlayer[0] = tf.layers.dense(X_ANN,npl,activation=actf,kernel_regularizer=reg) # Specify first layer\n",
    "\n",
    "    # Now specify layers 2 through (n_hidden), assuming the same activation function is used throughout\n",
    "    for el in range(1,n_hidden):\n",
    "        hlayer[el] = tf.layers.dense(hlayer[el-1],npl,activation=actf,kernel_regularizer=reg)\n",
    "\n",
    "    outlayer = tf.layers.dense(hlayer[n_hidden-1],dimof_output,activation=tf.nn.softmax)\n",
    "\n",
    "    cross_ent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true_ANN,logits=outlayer))\n",
    "    optimizer = tf.train.AdamOptimizer(lrate)\n",
    "    train_ANN = optimizer.minimize(cross_ent)\n",
    "\n",
    "    ## Train neural net for classification\n",
    "    init2 = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            #print(\"On Epoch {}\".format(epoch))\n",
    "\n",
    "            for step in range(spe):\n",
    "\n",
    "                sess.run(train_ANN,feed_dict={X_ANN:X_train_water_shuffled,y_true_ANN:y_train})\n",
    "\n",
    "        predictions = outlayer.eval(feed_dict={X_ANN:X_test_water_shuffled})\n",
    "        \n",
    "    model_num += 1\n",
    "    print(\"Model {}\".format(model_num))\n",
    "    # Evaluate accuracy on test set:\n",
    "    predictions_r = predictions.round(0)\n",
    "\n",
    "    # Compute test set accuracy\n",
    "    ANN_test_acc_waterchem_shuffled = np.sum(np.all(np.equal(predictions_r,y_test),axis=1))/y_test.shape[0]\n",
    "    #print(\"ANN testing accuracy is {:0.2f}% using SHUFFLED water chemistry variables only.\".format(ANN_test_acc_waterchem*100))\n",
    "    acc_array.append(ANN_test_acc_waterchem_shuffled)\n",
    "    acc_array = np.asarray(acc_array)\n",
    "    shuffled_accuracies[:,i] = acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display accuracies in all experiment in dataframe:\n",
    "acc_df = pd.DataFrame(columns = ['Variable Permutated'])\n",
    "acc_df['Variable Permutated'] = waterchem_var   \n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    col_name = (i+1 )\n",
    "    acc_df[col_name] = (shuffled_accuracies[:,i])*100\n",
    "\n",
    "# Finally, calculate average accuracy values:\n",
    "acc_base_df = pd.DataFrame(columns = ['Variable','Base Accuracy (%)'])\n",
    "acc_base_df['Variable'] = waterchem_var\n",
    "\n",
    "for i in acc_base_df.index:\n",
    "    acc_base_df.loc[[i],'Base Accuracy (%)'] = np.average(np.asarray(acc_df.loc[[i]].values[:,1:]),axis=1)\n",
    "\n",
    "acc_base_df['Base Accuracy (%)'] = acc_base_df.loc[:,'Base Accuracy (%)':'Base Accuracy (%)'].astype(float).round(1)\n",
    "acc_base_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 MDA Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_everything.drop(labels=['SampleID','NRR','NRF','SeRR','SeRF'],axis=1)\n",
    "y = pd.DataFrame(df_everything['SeRR'])\n",
    "n_feats = X.columns.shape[0]\n",
    "\n",
    "# Specify desired test fraction:\n",
    "test_frac = 0.4 # No hyperparameter selection, so no validation set\n",
    "\n",
    "# Obtain categorical values of y: (y is 0 if positive, 1 if negative)\n",
    "y['class'] = np.where(y['SeRR']>=0, 0, 1)\n",
    "y_class = y['class'].values\n",
    "C = np.unique(y_class).shape[0] # Number of classes\n",
    "\n",
    "y_onehot = np.eye(C)[y_class.astype(int)] # Convert categorical to one-hot-encoding\n",
    "y_onehot = y_onehot.reshape(y_class.shape[0],C) # Reshape into dimensions (n_t rows by C columns)\n",
    "\n",
    "model_num = 0\n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    ### STEP 0: Randomly split data into training and testing.\n",
    "\n",
    "    # Split into training and testing portions\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y_onehot,test_size=test_frac,shuffle=True)\n",
    "\n",
    "    ## Now grab the waterchem data input subset:\n",
    "    X_train_water = X_train.loc[:,'EBCT':'FBR3']\n",
    "    X_test_water = X_test.loc[:,'EBCT':'FBR3']\n",
    "\n",
    "    acc_array = []\n",
    "    for j in range(0,num_waterchem_var):\n",
    "    ### STEP 1: PERMUTATE THE SPECIFIC VARIABLE\n",
    "\n",
    "        # Select one of these variables to randomly permutate\n",
    "        permutate_var = waterchem_var[j]\n",
    "\n",
    "        ## For TRAINING SET:\n",
    "        # Shuffle the values of this variable arbitrarily:\n",
    "        values_to_shuffle = X_train_water.loc[:,permutate_var].values\n",
    "        values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "        # Put these shuffled values back in the original dataset\n",
    "        X_train_water_shuffled = X_train_water.copy()\n",
    "        X_train_water_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "        ## For TESTING SET:\n",
    "        # Shuffle the values of this variable arbitrarily:\n",
    "        values_to_shuffle = X_test_water.loc[:,permutate_var].values\n",
    "        values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "        # Put these shuffled values back in the original dataset\n",
    "        X_test_water_shuffled = X_test_water.copy()\n",
    "        X_test_water_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "    ### STEP 2: RE-TRAIN ANN USING SHUFFLED DATASET, THEN EVALUATE ACCURACY:\n",
    "\n",
    "        # Train ANN:\n",
    "        X_ANN = tf.placeholder(tf.float32,shape=[None,X_train_water_shuffled.shape[1]])\n",
    "        y_true_ANN = tf.placeholder(tf.float32,shape=[None,C])\n",
    "\n",
    "        # Specify hyperparameters\n",
    "        npl = 20 # Number of neurons per layer\n",
    "        n_hidden = 10 # Number of hidden layers\n",
    "        lrate = 0.01 # Learning rate for gradient descent\n",
    "        epochs = 10 # Total number of iterations\n",
    "        spe = 20 # Steps per epoch: One step represents one update to the cost function gradient\n",
    "        actf = tf.nn.relu # Activation function\n",
    "        alpha = 0.1 # Magnitude of regularizer\n",
    "        reg = tf.contrib.layers.l2_regularizer(scale=alpha) # Regularizer function\n",
    "\n",
    "        dimof_output = C # Dimension of y\n",
    "\n",
    "        ## Specify neural net architecture\n",
    "\n",
    "        # Use a dictionary setup to generalize number of layers\n",
    "        hlayer = dict()\n",
    "        hlayer[0] = tf.layers.dense(X_ANN,npl,activation=actf,kernel_regularizer=reg) # Specify first layer\n",
    "\n",
    "        # Now specify layers 2 through (n_hidden), assuming the same activation function is used throughout\n",
    "        for el in range(1,n_hidden):\n",
    "            hlayer[el] = tf.layers.dense(hlayer[el-1],npl,activation=actf,kernel_regularizer=reg)\n",
    "\n",
    "        outlayer = tf.layers.dense(hlayer[n_hidden-1],dimof_output,activation=tf.nn.softmax)\n",
    "\n",
    "        cross_ent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true_ANN,logits=outlayer))\n",
    "        optimizer = tf.train.AdamOptimizer(lrate)\n",
    "        train_ANN = optimizer.minimize(cross_ent)\n",
    "\n",
    "        ## Train neural net for classification\n",
    "        init2 = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init2)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                #print(\"On Epoch {}\".format(epoch))\n",
    "\n",
    "                for step in range(spe):\n",
    "\n",
    "                    sess.run(train_ANN,feed_dict={X_ANN:X_train_water_shuffled,y_true_ANN:y_train})\n",
    "\n",
    "            predictions = outlayer.eval(feed_dict={X_ANN:X_test_water_shuffled})\n",
    "        \n",
    "        model_num += 1\n",
    "        print(\"Model {}\".format(model_num))\n",
    "        # Evaluate accuracy on test set:\n",
    "        predictions_r = predictions.round(0)\n",
    "\n",
    "        # Compute test set accuracy\n",
    "        ANN_test_acc_waterchem_shuffled = np.sum(np.all(np.equal(predictions_r,y_test),axis=1))/y_test.shape[0]\n",
    "        #print(\"ANN testing accuracy is {:0.2f}% using SHUFFLED water chemistry variables only.\".format(ANN_test_acc_waterchem*100))\n",
    "        acc_array.append(ANN_test_acc_waterchem_shuffled)\n",
    "    acc_array = np.asarray(acc_array)\n",
    "    shuffled_accuracies[:,i] = acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display accuracies in all experiment in dataframe:\n",
    "\n",
    "acc_df = pd.DataFrame(columns = ['Variable Permutated'])\n",
    "acc_df['Variable Permutated'] = waterchem_var   \n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    col_name = (i+1 )\n",
    "    acc_df[col_name] = (shuffled_accuracies[:,i])*100\n",
    "    \n",
    "# Finally, calculate average MDA values:\n",
    "acc_avg_df = pd.DataFrame(columns = ['Variable Permutated','Avg Acc Increase (%)'])\n",
    "acc_avg_df['Variable Permutated'] = waterchem_var\n",
    "\n",
    "for i in acc_base_df.index:\n",
    "    acc_avg_df.loc[[i],'Avg Acc Increase (%)'] = np.average(np.asarray(acc_df.loc[[i]].values[:,1:]),axis=1) - acc_base_df.loc[[i],'Base Accuracy (%)']\n",
    "\n",
    "acc_avg_df['Avg Acc Increase (%)'] = acc_avg_df.loc[:,'Avg Acc Increase (%)':'Avg Acc Increase (%)'].astype(float).round(1)\n",
    "acc_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out the top waterchem and OTU variables, in terms of importance according to MDA:\n",
    "num_waterchem_smallest = 4\n",
    "\n",
    "acc_avg_df_waterchem = acc_avg_df[0:num_waterchem_var]\n",
    "\n",
    "top_waterchem = acc_avg_df_waterchem.nsmallest(num_waterchem_smallest, 'Avg Acc Increase (%)', keep='first')\n",
    "\n",
    "display(top_waterchem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MDA for water chem + representative OTUs from hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the clustered OTU data with waterchem data\n",
    "# Grab waterchem data\n",
    "df_waterchem = pd.read_csv(open('df_waterchem.csv','rb'),skip_blank_lines=True)\n",
    "\n",
    "# Append OTU data to the right\n",
    "df_everything_hierarch = pd.merge(df_waterchem,rep_OTUs_scaled, how='left', left_on=['SampleID'], right_on=['SampleID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab waterchem variables + representative OTU variables\n",
    "df_everything = df_everything_hierarch.drop(labels=['SampleID','NRR','NRF','SeRR','SeRF'],axis=1)\n",
    "\n",
    "# What are the relevant variables?\n",
    "everything_var = df_everything.columns\n",
    "\n",
    "# How many waterchem variables are there?\n",
    "num_everything_var = everything_var.shape[0]\n",
    "\n",
    "print(num_everything_var)\n",
    "print(everything_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of MDA experiments to perform:\n",
    "n_MDA_runs = 10\n",
    "\n",
    "shuffled_accuracies = np.zeros([num_everything_var,n_MDA_runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_everything\n",
    "y = pd.DataFrame(df_everything_hierarch['SeRR'])\n",
    "n_feats = X.columns.shape[0]\n",
    "\n",
    "# What are the variables in this dataset?\n",
    "hierarch_var = X.columns\n",
    "\n",
    "# How many hierarchical variables are there?\n",
    "num_hierarch_var = hierarch_var.shape[0]\n",
    "\n",
    "print(num_hierarch_var)\n",
    "print(hierarch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Specify desired test fraction:\n",
    "test_frac = 0.4 # No hyperparameter selection, so no validation set\n",
    "\n",
    "# Obtain categorical values of y: (y is 0 if positive, 1 if negative)\n",
    "y['class'] = np.where(y['SeRR']>=0, 0, 1)\n",
    "y_class = y['class'].values\n",
    "C = np.unique(y_class).shape[0] # Number of classes\n",
    "\n",
    "y_onehot = np.eye(C)[y_class.astype(int)] # Convert categorical to one-hot-encoding\n",
    "y_onehot = y_onehot.reshape(y_class.shape[0],C) # Reshape into dimensions (n_t rows by C columns)\n",
    "\n",
    "model_num = 0\n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    ### STEP 0: Randomly split data into training and testing.\n",
    "\n",
    "    # Split into training and testing portions\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y_onehot,test_size=test_frac,shuffle=True)\n",
    "\n",
    "    acc_array = []\n",
    "    for j in range(0,num_hierarch_var):\n",
    "    ### STEP 1: PERMUTATE THE SPECIFIC VARIABLE\n",
    "\n",
    "        # Select one of these variables to randomly permutate\n",
    "        permutate_var = hierarch_var[j]\n",
    "\n",
    "        ## For TRAINING SET:\n",
    "        # Shuffle the values of this variable arbitrarily:\n",
    "        values_to_shuffle = X_train.loc[:,permutate_var].values\n",
    "        values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "        # Put these shuffled values back in the original dataset\n",
    "        X_train_shuffled = X_train.copy()\n",
    "        X_train_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "        ## For TESTING SET:\n",
    "        # Shuffle the values of this variable arbitrarily:\n",
    "        values_to_shuffle = X_test.loc[:,permutate_var].values\n",
    "        values_to_shuffle = np.random.permutation(values_to_shuffle)\n",
    "\n",
    "        # Put these shuffled values back in the original dataset\n",
    "        X_test_shuffled = X_test.copy()\n",
    "        X_test_shuffled[permutate_var] = values_to_shuffle\n",
    "\n",
    "    ### STEP 2: RE-TRAIN ANN USING SHUFFLED DATASET, THEN EVALUATE ACCURACY:\n",
    "\n",
    "        # Train ANN:\n",
    "        X_ANN = tf.placeholder(tf.float32,shape=[None,X_train_shuffled.shape[1]])\n",
    "        y_true_ANN = tf.placeholder(tf.float32,shape=[None,C])\n",
    "\n",
    "        # Specify hyperparameters\n",
    "        npl = 20 # Number of neurons per layer\n",
    "        n_hidden = 10 # Number of hidden layers\n",
    "        lrate = 0.01 # Learning rate for gradient descent\n",
    "        epochs = 10 # Total number of iterations\n",
    "        spe = 20 # Steps per epoch: One step represents one update to the cost function gradient\n",
    "        actf = tf.nn.relu # Activation function\n",
    "        alpha = 0.1 # Magnitude of regularizer\n",
    "        reg = tf.contrib.layers.l2_regularizer(scale=alpha) # Regularizer function\n",
    "\n",
    "        dimof_output = C # Dimension of y\n",
    "\n",
    "        ## Specify neural net architecture\n",
    "\n",
    "        # Use a dictionary setup to generalize number of layers\n",
    "        hlayer = dict()\n",
    "        hlayer[0] = tf.layers.dense(X_ANN,npl,activation=actf,kernel_regularizer=reg) # Specify first layer\n",
    "\n",
    "        # Now specify layers 2 through (n_hidden), assuming the same activation function is used throughout\n",
    "        for el in range(1,n_hidden):\n",
    "            hlayer[el] = tf.layers.dense(hlayer[el-1],npl,activation=actf,kernel_regularizer=reg)\n",
    "\n",
    "        outlayer = tf.layers.dense(hlayer[n_hidden-1],dimof_output,activation=tf.nn.softmax)\n",
    "\n",
    "        cross_ent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true_ANN,logits=outlayer))\n",
    "        optimizer = tf.train.AdamOptimizer(lrate)\n",
    "        train_ANN = optimizer.minimize(cross_ent)\n",
    "\n",
    "        ## Train neural net for classification\n",
    "        init2 = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init2)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                # print(\"On Epoch {}\".format(epoch))\n",
    "\n",
    "                for step in range(spe):\n",
    "\n",
    "                    sess.run(train_ANN,feed_dict={X_ANN:X_train_shuffled,y_true_ANN:y_train})\n",
    "\n",
    "            predictions = outlayer.eval(feed_dict={X_ANN:X_test_shuffled})\n",
    "        \n",
    "        model_num += 1\n",
    "        print(\"Model {}\".format(model_num))\n",
    "        # Evaluate accuracy on test set:\n",
    "        predictions_r = predictions.round(0)\n",
    "\n",
    "        # Compute test set accuracy\n",
    "        ANN_test_acc_shuffled = np.sum(np.all(np.equal(predictions_r,y_test),axis=1))/y_test.shape[0]\n",
    "        #print(\"ANN testing accuracy is {:0.2f}% using SHUFFLED hierarch variables.\".format(ANN_test_acc_waterchem*100))\n",
    "        acc_array.append(ANN_test_acc_shuffled)\n",
    "    acc_array = np.asarray(acc_array)\n",
    "    shuffled_accuracies[:,i] = acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display accuracies in all experiment in dataframe:\n",
    "\n",
    "acc_df = pd.DataFrame(columns = ['Variable Permutated'])\n",
    "acc_df['Variable Permutated'] = waterchem_var   \n",
    "\n",
    "for i in range(0,n_MDA_runs):\n",
    "    col_name = (i+1 )\n",
    "    acc_df[col_name] = (shuffled_accuracies[:,i]-ANN_test_acc_waterchem)*100\n",
    "    \n",
    "# Finally, calculate average MDA values:\n",
    "acc_avg_df = pd.DataFrame(columns = ['Variable Permutated','Average Accuracy (%)'])\n",
    "acc_avg_df['Variable Permutated'] = waterchem_var\n",
    "\n",
    "for i in acc_avg_df.index:\n",
    "    acc_avg_df.loc[[i],'Average Accuracy (%)'] = np.average(np.asarray(acc_df.loc[[i]].values[:,1:]),axis=1)\n",
    "\n",
    "acc_avg_df['Average Accuracy (%)'] = acc_avg_df.loc[:,'Average Accuracy (%)':'Average Accuracy (%)'].astype(float).round(1)\n",
    "acc_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
